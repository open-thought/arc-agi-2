# ARC Ideas

## Classic transduction approach
- main advantage: TTT (program search/hypotheses filtering via gradient descent)
- data is king: scale high-quality synthetic riddle generation
	- aim for collisions with the private ARC test-set, most ARC-AGI-1 riddles were probably generated by Chollet himself, attack vector: "replicate Chollet" - I extracted the "core-ideas" behind ARC-1 with Sonnet and generated >1k derived riddle-generator, -verifier pairs (starting from core-idea descriptions and re-arc generators as seed-samples).
	- increase diversity, strive for new original ideas, use human in the loop for steering & filtering/ranking
- find best open-weights model for transduction fine-tuning
- catch up with experienced teams - gain experience how transduction generalizes (e.g. by training data of single re-arc generator or simple riddle sub-sets, e.g. generators with lowest number of source-lines)
- try finetuning a VLM, e.g. visual representation ARC boards (hope VLMs have advantages in recognizing 2D spatial relations)
- experiments/ablation around TTT, improving TTT (e.g. try to make use of the test-input board by testing for consistency)
- collect a list of augmentation techniques and test their effectiveness for TTT
- try to apply nanoGPT speedrun insights (e.g. Muon optimizer)


## ARC agent
- generate a ARC CoT "reasoning" dataset of transformation description based on synthetic riddle-generators as oracles
- train logic verifier / process reward model
- reflect with teacher model about failures -> generate data for back-tracking/self-correction
- generate descriptions of riddle boards (kind of riddle, in-out sizes, color-histogram, objects)
- search for similar riddles in synthetic riddle database and put them into context (RAG) for inspiration


## Combine induction & transduction
- estimate max llm based program generation throughput (e.g. non-naive tree-search with prefix-caching)
