# ARC Brainstorming/Ideas (Dec 2024)

## Classic transduction approach

- motivation: use TTT (program search/hypotheses filtering via gradient descent)
- data is king: scale high-quality synthetic riddle generation
	- aim for collisions with the private ARC test-set, most ARC-AGI-1 riddles were probably generated by Chollet himself, attack vector: "replicate Chollet" - already extracted the "[core-ideas](https://github.com/open-thought/arc-agi-2/blob/main/arc-1/annotated-re-arc/core_idea_samples.json)" behind ARC-1 with Sonnet and generated >1k derived riddle-generator+verifier pairs (open-ended generation starting from core-idea descriptions and re-arc generators as seed-samples).
	- maximize synthetic riddle diversity, e.g. by selecting in-context examples for generation cleverly and filtering outputs
	- train a model to guess the core-idea/concepts behind a riddle just from the riddle boards (output to be used as additional embedding for RAG)
	- strive for new original ideas, e.g. use human in the loop for steering & filtering/ranking of synthetic riddle generation
	- collect input & output of human feedback to capture the human-preferences (e.g. to filter out-of-human-distribution samples out)
- search best open-weights model for transduction fine-tuning (probably <= 8B)
- catch up with experienced teams: gain experience how transduction generalizes (e.g. by training data of single re-arc generator or simple riddle sub-sets, e.g. generators with lowest number of source-lines)
- try fine-tuning a VLM, e.g. visual representation ARC boards (vision encoders might have advantages in recognizing 2D spatial relations)
- collect a list of riddle board augmentation techniques and test their effectiveness for fine-tuning and during TTT (e.g. see [arcmentations](https://github.com/arc-community/arcmentations))
- run experiments around TTT: augmentations, freezing layers, training only FFN/KQV, different optimizers, regularization & hyper parameters
- check effectiveness of hallucination detection (via internal state classifier) and mitigation for ARC riddles (e.g. paper [FactCheckmate](https://arxiv.org/abs/2410.02899))
- use the test-input board for consistency-checking: predict test output & swap with training example -> verify that training example output is predicted correctly given test-input&output as training pair (check if this consistency-test can be used in the TTT objective).
- try to apply nanoGPT speedrun ([leaderboard](https://app.primeintellect.ai/speedrun/nanogpt)) insights (e.g. [Muon optimizer](https://github.com/KellerJordan/Muon))
  

## ARC agent

- generate an ARC CoT "reasoning" dataset of transformation description based on synthetic riddle-generator source-code as oracles
- train logic verifier / process reward model, use search-techniques to find correct result
- reflect with teacher model about failures -> generate data for back-tracking/self-correction
- generate descriptions of riddle boards (kind of riddle, in-out sizes, color-histogram, objects)
- search for similar riddles in synthetic riddle database and put them into context (RAG) "for inspiration"
  

## Combine induction & transduction

- estimate max llm based program generation throughput (e.g. non-naive tree-search with prefix-caching)
